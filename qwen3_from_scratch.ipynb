{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQLuHlbT9nnT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn  # Neural network modules like Linear, Embedding, etc.\n",
        "import torch.nn.functional as F  # Functional interface for operations like cross_entropy, silu, etc.\n",
        "from torch.utils.data import Dataset, DataLoader  # Base class and utilities for loading datasets\n",
        "from torch.cuda.amp import autocast, GradScaler  # ðŸ”„ Automatic Mixed Precision (AMP) tools for faster/lower-memory training\n",
        "\n",
        "import math  # Standard math operations (e.g. sqrt, exp, cos)\n",
        "import random  # Python's random number utilities (used for seeding)\n",
        "import numpy as np  # Numerical computing library, used for random seeding and general array ops\n",
        "\n",
        "from datasets import load_dataset  # ðŸ§ Hugging Face Datasets library for streaming large datasets\n",
        "from tqdm import tqdm  # â³ Progress bar visualization library, great for loops\n",
        "\n",
        "import time  # âŒ› Timing utilities, measuring time\n",
        "from transformers import AutoTokenizer  # ðŸ¤— Load pretrained tokenizers from HuggingFace with one line\n",
        "\n",
        "from dataclasses import dataclass  # ðŸ§± Define simple classes for configs with less boilerplate\n",
        "from typing import List, Optional  # âœï¸ Type hints for better readability and tooling\n",
        "\n",
        "import warnings  # âš ï¸ Suppress or handle warnings\n",
        "import os  # ðŸ—‚ï¸ File system operations (creating folders, path checking, etc.)\n",
        "import pickle  # ðŸ’¾ Python object serialization (used to save/load preprocessed datasets)\n",
        "\n",
        "warnings.filterwarnings('ignore')  # Silences warnings for cleaner outputs during training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3CczY5ms9nnU"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed: int = 42):\n",
        "    \"\"\"Set all random seeds for reproducibility\"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    print(f\"ðŸŒ± Set all seeds to {seed}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Buy5jS939nnV"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class ModelConfig:\n",
        "    # Model architecture\n",
        "    d_model: int = 384\n",
        "    n_heads: int = 8\n",
        "    n_layers: int = 6\n",
        "    d_ff: int = 1536\n",
        "    batch_size: int = 24\n",
        "    max_steps: int = 2000\n",
        "\n",
        "    # Qwen3-like parameters\n",
        "    n_kv_heads: int = 4  # For Grouped-Query Attention\n",
        "    sliding_window: int = 4096  # Set a large default, effectively disabling it unless specified\n",
        "    attention_bias: bool = False  # Qwen3 often sets this to False\n",
        "    rms_norm_eps: float = 1e-6  # Epsilon for RMSNorm\n",
        "\n",
        "    # Training parameters\n",
        "    gradient_accumulation_steps: int = 4\n",
        "    muon_lr: float = 0.01\n",
        "\n",
        "    # Data parameters\n",
        "    max_seq_len: int = 512\n",
        "    num_documents: int = 2000\n",
        "    max_tokens: int = 500000\n",
        "\n",
        "    # Evaluation\n",
        "    eval_every: int = 500\n",
        "    eval_steps: int = 100\n",
        "\n",
        "    # Regularization\n",
        "    weight_decay: float = 0.1\n",
        "    dropout: float = 0.1\n",
        "    grad_clip: float = 1.0\n",
        "\n",
        "    # Technical\n",
        "    use_amp: bool = True\n",
        "    vocab_size: Optional[int] = None\n",
        "\n",
        "    def __post_init__(self):\n",
        "        self.d_k = self.d_model // self.n_heads\n",
        "        assert self.d_model % self.n_heads == 0, \"d_model must be divisible by n_heads\"\n",
        "        assert self.n_heads % self.n_kv_heads == 0, \"n_heads must be divisible by n_kv_heads\"\n",
        "        self.n_kv_groups = self.n_heads // self.n_kv_heads\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yucDeNey9nnV"
      },
      "outputs": [],
      "source": [
        "def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep).\n",
        "    The hidden states go from (batch, num_key_value_heads, seqlen, head_dim)\n",
        "    to (batch, num_attention_heads, seqlen, head_dim)\n",
        "    \"\"\"\n",
        "    # Extract dimensions from input tensor\n",
        "    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n",
        "\n",
        "    # Early return if no repetition is needed\n",
        "    if n_rep == 1:\n",
        "        return hidden_states\n",
        "\n",
        "    # Add a new dimension at index 2 (after num_key_value_heads) and expand\n",
        "    # Shape transformation:\n",
        "    # (batch, num_key_value_heads, slen, head_dim)\n",
        "    # -> (batch, num_key_value_heads, 1, slen, head_dim) [via None indexing]\n",
        "    # -> (batch, num_key_value_heads, n_rep, slen, head_dim) [via expand]\n",
        "    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n",
        "\n",
        "    # Flatten the num_key_value_heads and n_rep dimensions together\n",
        "    # Final shape: (batch, num_key_value_heads * n_rep, slen, head_dim)\n",
        "    # This effectively repeats each key/value head n_rep times\n",
        "    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7gASpg2l9nnX"
      },
      "outputs": [],
      "source": [
        "class Qwen3Attention(nn.Module):\n",
        "    def __init__(self, config: ModelConfig):\n",
        "        super().__init__()\n",
        "        self.d_model = config.d_model\n",
        "        self.n_heads = config.n_heads\n",
        "        self.n_kv_heads = config.n_kv_heads\n",
        "        self.n_kv_groups = config.n_kv_groups\n",
        "        self.d_k = config.d_k\n",
        "\n",
        "        # Separate linear layers for Q, K, V\n",
        "        self.q_proj = nn.Linear(self.d_model, self.n_heads * self.d_k, bias=config.attention_bias)\n",
        "        self.k_proj = nn.Linear(self.d_model, self.n_kv_heads * self.d_k, bias=config.attention_bias)\n",
        "        self.v_proj = nn.Linear(self.d_model, self.n_kv_heads * self.d_k, bias=config.attention_bias)\n",
        "        self.w_o = nn.Linear(self.d_model, self.d_model, bias=False)\n",
        "\n",
        "        # QK-Normalization layers\n",
        "        self.q_norm = nn.RMSNorm(self.d_k, eps=config.rms_norm_eps)\n",
        "        self.k_norm = nn.RMSNorm(self.d_k, eps=config.rms_norm_eps)\n",
        "\n",
        "        self.rotary = Rotary(self.d_k, config.max_seq_len)\n",
        "        self.dropout = config.dropout\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len = x.size(0), x.size(1)\n",
        "\n",
        "        # 1. Project Q, K, V separately\n",
        "        q = self.q_proj(x)\n",
        "        k = self.k_proj(x)\n",
        "        v = self.v_proj(x)\n",
        "\n",
        "        # 2. Reshape into heads\n",
        "        q = q.view(batch_size, seq_len, self.n_heads, self.d_k)\n",
        "        k = k.view(batch_size, seq_len, self.n_kv_heads, self.d_k)\n",
        "        v = v.view(batch_size, seq_len, self.n_kv_heads, self.d_k)\n",
        "\n",
        "        # 3. Apply QK-Norm\n",
        "        q = self.q_norm(q)\n",
        "        k = self.k_norm(k)\n",
        "\n",
        "        # 4. Apply RoPE\n",
        "        # Transpose to (batch, seq_len, n_heads, d_k) -> (batch, n_heads, seq_len, d_k) for rotary\n",
        "        q = self.rotary(q.permute(0, 2, 1, 3)).permute(0, 2, 1, 3)\n",
        "        k = self.rotary(k.permute(0, 2, 1, 3)).permute(0, 2, 1, 3)\n",
        "\n",
        "        # Transpose for attention: (batch, seq_len, n_heads, d_k) -> (batch, n_heads, seq_len, d_k)\n",
        "        Q = q.transpose(1, 2)\n",
        "        K = k.transpose(1, 2)\n",
        "        V = v.transpose(1, 2)\n",
        "\n",
        "        # 5. Repeat K and V heads for GQA\n",
        "        K = repeat_kv(K, self.n_kv_groups)\n",
        "        V = repeat_kv(V, self.n_kv_groups)\n",
        "\n",
        "        # 6. Scaled Dot-Product Attention\n",
        "        attn_output = F.scaled_dot_product_attention(\n",
        "            Q, K, V, is_causal=True, dropout_p=self.dropout if self.training else 0.0\n",
        "        )\n",
        "\n",
        "        # 7. Reshape and final projection\n",
        "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
        "        return self.w_o(attn_output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05g5swnW9nnX"
      },
      "outputs": [],
      "source": [
        "class SwiGLUFeedForward(nn.Module):\n",
        "    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.gate_proj = nn.Linear(d_model, d_ff, bias=False)\n",
        "        self.down_proj = nn.Linear(d_ff, d_model, bias=False)\n",
        "        self.up_proj = nn.Linear(d_model, d_ff, bias=False)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Implementation of the SwiGLU activation function\n",
        "        # F.silu is the Swish activation function\n",
        "        activated_x = F.silu(self.gate_proj(x)) * self.up_proj(x)\n",
        "        return self.down_proj(self.dropout(activated_x))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxaNJjGQ9nnX"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, config: ModelConfig):  # Pass the entire config object\n",
        "        super().__init__()\n",
        "        self.attention = Qwen3Attention(config)\n",
        "        self.feed_forward = SwiGLUFeedForward(config.d_model, config.d_ff, config.dropout)\n",
        "        self.norm1 = nn.RMSNorm(config.d_model, eps=config.rms_norm_eps)\n",
        "        self.norm2 = nn.RMSNorm(config.d_model, eps=config.rms_norm_eps)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        attn_out = self.attention(self.norm1(x))\n",
        "        x = x + self.dropout(attn_out)\n",
        "        ff_out = self.feed_forward(self.norm2(x))\n",
        "        x = x + self.dropout(ff_out)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p6Ov0uO29nnY"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model: nn.Module, val_loader: DataLoader, config: ModelConfig):\n",
        "    \"\"\"Evaluate model performance\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_tokens = 0\n",
        "    total_correct = 0\n",
        "\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient computation for evaluation (saves memory and computation)\n",
        "        for i, (x, y) in enumerate(val_loader):\n",
        "            # Stop evaluation after specified number of steps to limit eval time\n",
        "            if i >= config.eval_steps:\n",
        "                break\n",
        "\n",
        "            # Move input sequences (x) and target sequences (y) to GPU/device\n",
        "            x, y = x.to(device), y.to(device)\n",
        "\n",
        "            # Use automatic mixed precision if enabled (faster training with minimal accuracy loss)\n",
        "            with autocast(enabled=config.use_amp):\n",
        "                # Forward pass: get model predictions (logits) for input sequence\n",
        "                logits = model(x)\n",
        "\n",
        "                # Calculate cross-entropy loss between predictions and targets\n",
        "                # Reshape to (batch_size * seq_len, vocab_size) and (batch_size * seq_len,)\n",
        "                # for proper cross-entropy computation across all token positions\n",
        "                loss = F.cross_entropy(logits.view(-1, config.vocab_size), y.view(-1))\n",
        "\n",
        "            # Accumulate total loss weighted by number of tokens in this batch\n",
        "            total_loss += loss.item() * y.numel()\n",
        "            # Keep track of total number of tokens processed\n",
        "            total_tokens += y.numel()\n",
        "\n",
        "            # Get predicted token IDs by taking argmax over vocabulary dimension\n",
        "            predictions = logits.argmax(dim=-1)\n",
        "            # Count correct predictions for accuracy calculation\n",
        "            total_correct += (predictions == y).sum().item()\n",
        "\n",
        "    avg_loss = total_loss / total_tokens\n",
        "    accuracy = total_correct / total_tokens\n",
        "    perplexity = math.exp(min(avg_loss, 20))\n",
        "\n",
        "    model.train()\n",
        "    return {'val_loss': avg_loss, 'val_accuracy': accuracy, 'val_perplexity': perplexity}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o3gfmjW29nnY"
      },
      "outputs": [],
      "source": [
        "def setup_muon_optimizer(model: nn.Module, config: ModelConfig):\n",
        "    \"\"\"Setup Muon optimizer with hybrid approach\"\"\"\n",
        "    muon_params = []\n",
        "    adamw_params = []\n",
        "\n",
        "    for name, param in model.named_parameters():\n",
        "        if (param.ndim == 2 and\n",
        "            'token_embedding' not in name and\n",
        "            'norm' not in name and\n",
        "            param.requires_grad):\n",
        "            muon_params.append(param)\n",
        "        else:\n",
        "            adamw_params.append(param)\n",
        "\n",
        "    print(f\"  Muon parameters: {sum(p.numel() for p in muon_params):,}\")\n",
        "    print(f\"  AdamW parameters: {sum(p.numel() for p in adamw_params):,}\")\n",
        "\n",
        "    muon_optimizer = Muon(muon_params, lr=config.muon_lr, momentum=0.95)\n",
        "    adamw_optimizer = torch.optim.AdamW(adamw_params, lr=config.muon_lr*0.1, weight_decay=config.weight_decay)\n",
        "\n",
        "    return [muon_optimizer, adamw_optimizer]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RT4qQBd_9nnY"
      },
      "outputs": [],
      "source": [
        "def train_model(config: ModelConfig, train_loader: DataLoader, val_loader: DataLoader):\n",
        "    \"\"\"Train the model with Muon optimizer\"\"\"\n",
        "    print(f\"\\nðŸš€ Training Small model with Muon optimizer\")\n",
        "\n",
        "    # Initialize model\n",
        "    set_seed(42)\n",
        "    model = MinimalLLM(config)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"  ðŸ“Š Total parameters: {total_params:,}\")\n",
        "\n",
        "    # Setup optimizers\n",
        "    optimizers = setup_muon_optimizer(model, config)\n",
        "\n",
        "    # Learning rate schedule\n",
        "    schedulers = []\n",
        "    for optimizer in optimizers:\n",
        "        warmup_steps = config.max_steps // 20\n",
        "        def lr_lambda(step):\n",
        "            if step < warmup_steps:\n",
        "                return step / warmup_steps\n",
        "            else:\n",
        "                progress = (step - warmup_steps) / (config.max_steps - warmup_steps)\n",
        "                return 0.1 + 0.9 * 0.5 * (1 + math.cos(math.pi * progress))\n",
        "\n",
        "        scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "        schedulers.append(scheduler)\n",
        "\n",
        "    scaler = GradScaler() if config.use_amp else None\n",
        "\n",
        "    # Training loop\n",
        "    model.train()\n",
        "    step = 0\n",
        "    start_time = time.time()\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    pbar = tqdm(total=config.max_steps, desc=\"Training\")\n",
        "\n",
        "    while step < config.max_steps:\n",
        "        for batch_idx, (x, y) in enumerate(train_loader):\n",
        "            if step >= config.max_steps:\n",
        "                break\n",
        "\n",
        "            x, y = x.to(device), y.to(device)\n",
        "\n",
        "            # Forward pass with gradient accumulation\n",
        "            if config.use_amp:\n",
        "                with autocast():\n",
        "                    logits = model(x)\n",
        "                    loss = F.cross_entropy(logits.view(-1, config.vocab_size), y.view(-1))\n",
        "                    loss = loss / config.gradient_accumulation_steps\n",
        "                scaler.scale(loss).backward()\n",
        "            else:\n",
        "                logits = model(x)\n",
        "                loss = F.cross_entropy(logits.view(-1, config.vocab_size), y.view(-1))\n",
        "                loss = loss / config.gradient_accumulation_steps\n",
        "                loss.backward()\n",
        "\n",
        "            # Optimizer step after accumulation\n",
        "            if (step + 1) % config.gradient_accumulation_steps == 0:\n",
        "                if config.use_amp:\n",
        "                    for optimizer in optimizers:\n",
        "                        scaler.unscale_(optimizer)\n",
        "                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_clip)\n",
        "\n",
        "                    for optimizer in optimizers:\n",
        "                        scaler.step(optimizer)\n",
        "                        optimizer.zero_grad()\n",
        "                    for scheduler in schedulers:\n",
        "                        scheduler.step()\n",
        "                    scaler.update()\n",
        "                else:\n",
        "                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_clip)\n",
        "                    for optimizer in optimizers:\n",
        "                        optimizer.step()\n",
        "                        optimizer.zero_grad()\n",
        "                    for scheduler in schedulers:\n",
        "                        scheduler.step()\n",
        "\n",
        "            # Logging\n",
        "            if step % 10 == 0:\n",
        "                with torch.no_grad():\n",
        "                    predictions = logits.argmax(dim=-1)\n",
        "                    accuracy = (predictions == y).float().mean().item()\n",
        "                    current_loss = loss.item() * config.gradient_accumulation_steps\n",
        "                    perplexity = math.exp(min(current_loss, 20))\n",
        "\n",
        "                pbar.set_postfix({\n",
        "                    'loss': f'{current_loss:.4f}',\n",
        "                    'acc': f'{accuracy:.3f}',\n",
        "                    'ppl': f'{perplexity:.1f}',\n",
        "                    'lr': f'{optimizers[0].param_groups[0][\"lr\"]:.2e}'\n",
        "                })\n",
        "\n",
        "            # Evaluation\n",
        "            if step % config.eval_every == 0 and step > 0:\n",
        "                eval_metrics = evaluate_model(model, val_loader, config)\n",
        "                print(f\"\\nStep {step}: Val Loss: {eval_metrics['val_loss']:.4f}, \"\n",
        "                      f\"Val Acc: {eval_metrics['val_accuracy']:.4f}, \"\n",
        "                      f\"Val PPL: {eval_metrics['val_perplexity']:.2f}\")\n",
        "\n",
        "                if eval_metrics['val_loss'] < best_val_loss:\n",
        "                    best_val_loss = eval_metrics['val_loss']\n",
        "                    # Save best model\n",
        "                    torch.save({\n",
        "                        'model_state_dict': model.state_dict(),\n",
        "                        'config': config,\n",
        "                        'step': step,\n",
        "                        'best_val_loss': best_val_loss,\n",
        "                        'final_metrics': eval_metrics\n",
        "                    }, 'best_model.pt')\n",
        "                    print(f\"ðŸ’¾ Saved best model with val_loss: {best_val_loss:.4f}\")\n",
        "\n",
        "            step += 1\n",
        "            if step % 10 == 0:\n",
        "                pbar.update(10)\n",
        "\n",
        "    pbar.close()\n",
        "\n",
        "    training_time = time.time() - start_time\n",
        "    print(f\"  â±ï¸ Training completed in {training_time:.1f} seconds\")\n",
        "\n",
        "    # Final evaluation\n",
        "    final_eval = evaluate_model(model, val_loader, config)\n",
        "    print(f\"  ðŸ“Š Final - Loss: {final_eval['val_loss']:.4f}, \"\n",
        "          f\"Acc: {final_eval['val_accuracy']:.4f}, PPL: {final_eval['val_perplexity']:.2f}\")\n",
        "\n",
        "    # Save final model\n",
        "    torch.save({\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'config': config,\n",
        "        'step': step,\n",
        "        'final_metrics': final_eval\n",
        "    }, 'final_model.pt')\n",
        "    print(f\"ðŸ’¾ Saved final model to final_model.pt\")\n",
        "\n",
        "    return model, final_eval\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ufOMN_19nnY"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Check system\n",
        "    print(f\"ðŸ” Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
        "        print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "\n",
        "    # Set seed\n",
        "    set_seed(42)\n",
        "\n",
        "    # Create config for Small model\n",
        "    config = ModelConfig()\n",
        "    print(f\"\\nðŸ“‹ Model Configuration:\")\n",
        "    print(f\"   Architecture: {config.d_model}d, {config.n_layers}L, {config.n_heads}H, {config.d_ff}ff\")\n",
        "    print(f\"   Training: {config.max_steps} steps, batch size {config.batch_size}\")\n",
        "    print(f\"   Data: {config.max_tokens:,} tokens, seq_len {config.max_seq_len}\")\n",
        "\n",
        "    # Load data\n",
        "    texts, tokenizer, tokens = load_and_cache_data(config)\n",
        "    dataset = TextTokenDataset(tokens, config.max_seq_len)\n",
        "\n",
        "    # Train/val split\n",
        "    val_size = len(dataset) // 10\n",
        "    train_size = len(dataset) - val_size\n",
        "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
        "        dataset, [train_size, val_size], generator=torch.Generator().manual_seed(42)\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    print(f\"ðŸ“Š Dataset: {len(train_dataset)} train, {len(val_dataset)} val samples\")\n",
        "\n",
        "    # Train model\n",
        "    start_time = time.time()\n",
        "    model, final_metrics = train_model(config, train_loader, val_loader)\n",
        "    total_time = time.time() - start_time\n",
        "\n",
        "    print(f\"\\nðŸŽ‰ TRAINING COMPLETED!\")\n",
        "    print(f\"â±ï¸ Total time: {total_time/60:.1f} minutes\")\n",
        "    print(f\"ðŸ† Final Results:\")\n",
        "    print(f\"   Validation Loss: {final_metrics['val_loss']:.4f}\")\n",
        "    print(f\"   Validation Accuracy: {final_metrics['val_accuracy']:.4f}\")\n",
        "    print(f\"   Validation Perplexity: {final_metrics['val_perplexity']:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_trained_model(model_path: str = \"final_model.pt\"):\n",
        "    \"\"\"Load a trained model from checkpoint\"\"\"\n",
        "    print(f\" Loading model from {model_path}\")\n",
        "\n",
        "    # Add ModelConfig to safe globals for PyTorch 2.6+\n",
        "    from torch.serialization import add_safe_globals\n",
        "    add_safe_globals([ModelConfig])\n",
        "\n",
        "    try:\n",
        "        checkpoint = torch.load(model_path, map_location='cpu')\n",
        "        config = checkpoint['config']\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ Error loading with weights_only=True, trying with weights_only=False...\")\n",
        "        checkpoint = torch.load(model_path, map_location='cpu', weights_only=False)\n",
        "        config = checkpoint['config']\n",
        "\n",
        "    # Create model with same config\n",
        "    model = MinimalLLM(config)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    print(f\"âœ… Model loaded successfully\")\n",
        "    print(f\"   Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "    print(f\"   Device: {device}\")\n",
        "\n",
        "    return model, config"
      ],
      "metadata": {
        "id": "jkJxa1P6Bz2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model: nn.Module, tokenizer, prompt: str, max_length: int = 100,\n",
        "                 temperature: float = 0.8, top_k: int = 50, top_p: float = 0.9):\n",
        "    \"\"\"Generate text using the trained model\"\"\"\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    # Tokenize prompt\n",
        "    input_ids = tokenizer.encode(prompt, add_special_tokens=False, return_tensors='pt').to(device)\n",
        "\n",
        "    generated_ids = input_ids.clone()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_length):\n",
        "            # Get model predictions\n",
        "            logits = model(generated_ids)\n",
        "            next_token_logits = logits[0, -1, :] / temperature\n",
        "\n",
        "            # Apply top-k filtering\n",
        "            if top_k > 0:\n",
        "                top_k_logits, top_k_indices = torch.topk(next_token_logits, top_k)\n",
        "                next_token_logits = torch.full_like(next_token_logits, float('-inf'))\n",
        "                next_token_logits[top_k_indices] = top_k_logits\n",
        "\n",
        "            # Apply top-p (nucleus) filtering\n",
        "            if top_p < 1.0:\n",
        "                sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)\n",
        "                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "                sorted_indices_to_remove = cumulative_probs > top_p\n",
        "                sorted_indices_to_remove[1:] = sorted_indices_to_remove[:-1].clone()\n",
        "                sorted_indices_to_remove[0] = 0\n",
        "                indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
        "                next_token_logits[indices_to_remove] = float('-inf')\n",
        "\n",
        "            # Sample next token\n",
        "            probs = F.softmax(next_token_logits, dim=-1)\n",
        "            next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "            # Append to generated sequence - FIX: ensure same dimensions\n",
        "            next_token = next_token.unsqueeze(0)  # Add batch dimension\n",
        "            generated_ids = torch.cat([generated_ids, next_token], dim=1)\n",
        "\n",
        "            # Stop if we reach the end token\n",
        "            if next_token.item() == tokenizer.eos_token_id:\n",
        "                break\n",
        "\n",
        "    # Decode the generated text\n",
        "    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "    return generated_text"
      ],
      "metadata": {
        "id": "SPJIgtbIB0_W"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "ZH1RhXtTSyvG"
      ]
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}